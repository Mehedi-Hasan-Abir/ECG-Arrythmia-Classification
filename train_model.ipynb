{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-da885068d386>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-da885068d386>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    $conda activate tf-gpu\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pip install opencv \n",
    "#jupyter notebook --generate-config\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "#!pip uninstall tensorflow\n",
    "#!pip install keras\n",
    "#!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9360990827473388415\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1477633639\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17534435418103511860\n",
      "physical_device_desc: \"device: 0, name: GeForce GT 730, pci bus id: 0000:01:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save in :thesis/results/\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "elu_15 (ELU)                 (None, 126, 126, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 126, 126, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 124, 124, 64)      36928     \n",
      "_________________________________________________________________\n",
      "elu_16 (ELU)                 (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 124, 124, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "elu_17 (ELU)                 (None, 60, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 60, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 58, 58, 128)       147584    \n",
      "_________________________________________________________________\n",
      "elu_18 (ELU)                 (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 58, 58, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 29, 29, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 27, 27, 256)       295168    \n",
      "_________________________________________________________________\n",
      "elu_19 (ELU)                 (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "elu_20 (ELU)                 (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 25, 25, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              75499520  \n",
      "_________________________________________________________________\n",
      "elu_21 (ELU)                 (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 16392     \n",
      "=================================================================\n",
      "Total params: 76,673,096\n",
      "Trainable params: 76,667,208\n",
      "Non-trainable params: 5,888\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "16326/64286 [======>.......................] - ETA: 24:49:49 - loss: 1.1042 - accuracy: 0.6969"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import model\n",
    "import callbacks \n",
    "from callbacks import Step\n",
    "from model import proposed_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "def plot_history(history, result_dir):\n",
    "    plt.plot(history.history['acc'], marker='.')\n",
    "    plt.plot(history.history['val_acc'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend(['acc', 'val_acc'], loc='lower right')\n",
    "    plt.savefig(os.path.join(result_dir, 'model_accuracy.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    plt.savefig(os.path.join(result_dir, 'model_loss.png'))\n",
    "    plt.close()\n",
    "def save_history(history, result_dir):\n",
    "    loss = history.history['loss']\n",
    "    acc = history.history['acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    nb_epoch = len(acc)\n",
    "\n",
    "    with open(os.path.join(result_dir, 'result.txt'), 'w') as fp:\n",
    "        fp.write('epoch\\tloss\\tacc\\tval_loss\\tval_acc\\n')\n",
    "        for i in range(nb_epoch):\n",
    "            fp.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "                i, loss[i], acc[i], val_loss[i], val_acc[i]))\n",
    "        fp.close()\n",
    "\n",
    "def process_batch(lines,img_path,inputH,inputW,train=True):\n",
    "    imagew = 128\n",
    "    imageh = 128\n",
    "    num = len(lines)\n",
    "    batch = np.zeros((num, imagew, imageh, 3), dtype='float32')\n",
    "\n",
    "\n",
    "    labels = np.zeros(num, dtype='int')\n",
    "    for i in range(num):\n",
    "        path = lines[i].split(' ')[0]\n",
    "        label = lines[i].split(' ')[-1]\n",
    "\n",
    "        label = label.strip('\\n')\n",
    "        label = int(label)\n",
    "\n",
    "        img = os.path.join(img_path, path)\n",
    "\n",
    "        if train:\n",
    "            crop_x = random.randint(0, np.max([0, imagew-inputW]))\n",
    "            crop_y = random.randint(0, np.max([0, imageh-inputH]))\n",
    "            is_flip = random.randint(0, 1)\n",
    "\n",
    "\n",
    "            image = cv2.imread(img)\n",
    "\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = image[crop_y:crop_y + inputH, crop_x:crop_x + inputW, :]\n",
    "            image = cv2.resize(image, (imagew, imageh))\n",
    "\n",
    "            if is_flip == 1:\n",
    "                image = cv2.flip(image, 1)\n",
    "\n",
    "            #batch[i][:][:][:] = image[crop_y:crop_y + inputH,crop_x:crop_x + inputW, :]\n",
    "            batch[i] = image\n",
    "            labels[i] = label\n",
    "        else:\n",
    "            image = cv2.imread(img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "            Hshmean = int(np.round(np.max([0, np.round((imageh-inputH)/2)])))\n",
    "            Wshmean = int(np.round(np.max([0, np.round((imagew-inputW)/2)])))\n",
    "            image = image[Hshmean:Hshmean+inputH,Wshmean:Wshmean+inputW, :]\n",
    "            image = cv2.resize(image, (imagew, imageh))\n",
    "\n",
    "            batch[i] = image\n",
    "            labels[i] = label\n",
    "\n",
    "    return batch, labels\n",
    "\n",
    "def generator_train_batch( train_txt, batch_size, num_classes, img_path, inputH, inputW ):\n",
    "    ff = open(train_txt, 'r')\n",
    "    lines = ff.readlines()\n",
    "    num = len(lines)\n",
    "    while True:\n",
    "        new_line = []\n",
    "        index = [n for n in range(num)]\n",
    "        random.shuffle(index)\n",
    "        for m in range(num):\n",
    "            new_line.append(lines[index[m]])\n",
    "\n",
    "        for i in range(int(num/batch_size)):\n",
    "            a = i*batch_size\n",
    "            b = (i+1)*batch_size\n",
    "            x_train, x_labels = process_batch(new_line[a:b], img_path, inputH, inputW, train=True)\n",
    "            y = np_utils.to_categorical(np.array(x_labels), num_classes)\n",
    "            yield x_train, y\n",
    "\n",
    "def generator_val_batch(val_txt,batch_size,num_classes,img_path,inputH,inputW):\n",
    "    f = open(val_txt, 'r')\n",
    "    lines = f.readlines()\n",
    "    num = len(lines)\n",
    "    while True:\n",
    "        new_line = []\n",
    "        index = [n for n in range(num)]\n",
    "        random.shuffle(index)\n",
    "        for m in range(num):\n",
    "            new_line.append(lines[index[m]])\n",
    "        for i in range(int(num / batch_size)):\n",
    "            a = i * batch_size\n",
    "            b = (i + 1) * batch_size\n",
    "            y_test,y_labels = process_batch(new_line[a:b],img_path,inputH,inputW,train=False)\n",
    "            y = np_utils.to_categorical(np.array(y_labels), num_classes)\n",
    "            yield y_test, y\n",
    "\n",
    "def generator_train_batch_proposed( new_lines, k, batch_size, num_classes, img_path, inputH, inputW ):\n",
    "\n",
    "    val_set = 0\n",
    "    while True:\n",
    "        if val_set >= k:\n",
    "            val_set = 0\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        new_line = []\n",
    "        for i in range(len(new_lines)):\n",
    "            if val_set != i:\n",
    "                new_line += new_lines[i]\n",
    "\n",
    "        num = len(new_line)\n",
    "        random.shuffle(new_line)\n",
    "        for i in range(int(num/batch_size)):\n",
    "            a = i*batch_size\n",
    "            b = (i+1)*batch_size\n",
    "            x_train, x_labels = process_batch(new_line[a:b], img_path, inputH, inputW, train=True)\n",
    "            y = np_utils.to_categorical(np.array(x_labels), num_classes)\n",
    "            yield x_train, y\n",
    "        val_set += 1\n",
    "\n",
    "def generator_val_batch_proposed(new_lines, k, batch_size, num_classes, img_path, inputH, inputW):\n",
    "\n",
    "\n",
    "    val_set = 0\n",
    "    while True:\n",
    "        if val_set >= k:\n",
    "            val_set = 0\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        new_line = new_lines[val_set]\n",
    "        num = len(new_lines)\n",
    "        random.shuffle(new_line)\n",
    "\n",
    "        for i in range(int(num / batch_size)):\n",
    "            a = i * batch_size\n",
    "            b = (i + 1) * batch_size\n",
    "            y_test,y_labels = process_batch(new_line[a:b],img_path,inputH,inputW,train=False)\n",
    "            y = np_utils.to_categorical(np.array(y_labels), num_classes)\n",
    "            yield y_test, y\n",
    "        val_set += 1\n",
    "\n",
    "def main():\n",
    "    proposed = True\n",
    "    if proposed:\n",
    "        outputdir = 'thesis/results/'\n",
    "        if os.path.isdir(outputdir):\n",
    "            print('save in :'+outputdir)\n",
    "        else:\n",
    "            os.makedirs(outputdir)\n",
    "\n",
    "        train_img_path = 'C:/thesis/MIT-BIH_AD/'\n",
    "        train_file = 'C:/thesis//MIT-BIH_AD_train.txt'\n",
    "        num_classes = 8\n",
    "        k = 10\n",
    "\n",
    "\n",
    "        f1 = open(train_file, 'r')      #########\n",
    "        lines = f1.readlines()\n",
    "        f1.close()\n",
    "\n",
    "        train_samples = len(lines)\n",
    "        val_samples = len(lines)//k\n",
    "\n",
    "        num = len(lines)\n",
    "        new_lines = []\n",
    "        index = [n for n in range(num)]\n",
    "        random.shuffle(index)\n",
    "        for m in range(num):\n",
    "            new_lines.append(lines[index[m]])\n",
    "\n",
    "        lines = new_lines\n",
    "        temp = []\n",
    "        new_lines = []\n",
    "        for i in range(num):\n",
    "            if i % val_samples == 0:\n",
    "                temp = []\n",
    "                new_lines.append(temp)\n",
    "            temp.append(lines[i])\n",
    "\n",
    "        batch_size = 1\n",
    "        epochs = 20\n",
    "        input_h = 96\n",
    "        input_w = 96\n",
    "\n",
    "        model = proposed_model()\n",
    "\n",
    "\n",
    "        lr = .00001\n",
    "        adam = Adam(lr=lr)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        #print(\"Lines------>\",new_lines)\n",
    "        \n",
    "        history = model.fit_generator(generator_train_batch_proposed(new_lines, k, batch_size, num_classes, train_img_path, input_h, input_w),\n",
    "                                      steps_per_epoch=train_samples // batch_size,\n",
    "                                      epochs=epochs,\n",
    "                                      callbacks=[Step()],\n",
    "                                      validation_data=generator_val_batch_proposed(new_lines, k, batch_size, num_classes, train_img_path, input_h, input_w),\n",
    "                                      validation_steps=val_samples // batch_size,\n",
    "                                      verbose=1)\n",
    "        plot_history(history, outputdir)\n",
    "        save_history(history, outputdir)\n",
    "        model.save_weights(outputdir+'proposed_model_{}.h5'.format(proposed))\n",
    "    else:\n",
    "        outputdir = 'thesis/results/first_attempt/'\n",
    "        if os.path.isdir(outputdir):\n",
    "            print('save in :' + outputdir)\n",
    "        else:\n",
    "            os.makedirs(outputdir)\n",
    "\n",
    "        train_img_path = 'C:/thesis/MIT-BIH_AD/'\n",
    "        test_img_path = 'C:/thesis/MIT-BIH_AD/'\n",
    "        train_file = 'C:/thesis/MIT-BIH_AD_train.txt'\n",
    "        test_file = 'C:/thesis/MIT-BIH_AD_val.txt'\n",
    "        num_classes = 8\n",
    "\n",
    "        f1 = open(train_file, 'r')\n",
    "        f2 = open(test_file, 'r')\n",
    "        lines = f1.readlines()\n",
    "        \n",
    "        f1.close()\n",
    "        train_samples = len(lines)\n",
    "        lines = f2.readlines()\n",
    "        f2.close()\n",
    "        val_samples = len(lines)\n",
    "\n",
    "        batch_size = 1\n",
    "        epochs = 20\n",
    "        input_h = 96\n",
    "        input_w = 96\n",
    "\n",
    "        model = proposed_model()\n",
    "\n",
    "        lr = 0.0001\n",
    "        adam = Adam(lr=lr)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        #print(\"Lines------>\",lines[0])\n",
    "        history = model.fit_generator(\n",
    "            generator_train_batch(train_file, batch_size, num_classes, train_img_path, input_h, input_w),\n",
    "            steps_per_epoch=train_samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=[Step()],\n",
    "            validation_data=generator_val_batch(test_file, batch_size, num_classes, test_img_path, input_h, input_w),\n",
    "            validation_steps=val_samples // batch_size,\n",
    "            verbose=1)\n",
    "        plot_history(history, outputdir)\n",
    "        save_history(history, outputdir)\n",
    "        model.save_weights(outputdir+'proposed_model_{}.h5'.format(proposed))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
